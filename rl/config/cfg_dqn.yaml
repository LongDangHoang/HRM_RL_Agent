# ARC training config

defaults:
  - arch_exclude_data_dependent: hrm_v1_dqn.yaml
  - dataset: dataset_cfg.yaml
  - _self_

max_training_steps: 10000

lr: 1e-4
lr_min_ratio: 1.0
lr_warmup_steps: 10000

# Standard hyperparameter settings for LM, as used in Llama
beta1: 0.9
beta2: 0.95
weight_decay: 0.0
puzzle_emb_weight_decay: 0.1

# Hyperparams - Puzzle embeddings training
puzzle_emb_lr: 1e-2

# instrumentation - wandb
project_name: hrm-rl-dqn-maze-full-observability
resume_from_run: null
log_wandb: False

# evaluation and time keeping
model_checkpoint_every_minutes: 45
evaluate_every_minutes: 45

# whether to only reset z_H and z_L on episode reset
use_last_hidden_state_to_seed_next_environment_step: True

# target estimator, DQN with TD-0
discount_factor: 0.95
start_eps: 1
end_eps: 0.33
# eps_decay_steps: ${max_training_steps}  # linear decay over the whole training
eps_decay_steps: 300000
use_target_network: True
target_network_decay_factor: 0.999 # roughly equivalent to a full replace in 1 / (1 - value) steps