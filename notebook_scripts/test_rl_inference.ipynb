{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from rl.agent import (\n",
    "    HRMQNetTrainingConfig,\n",
    ")\n",
    "from rl.dataset import MiniHackFullObservationSimpleEnvironmentDataset\n",
    "from rl.interfaces import evaluating_net_context\n",
    "from rl.environment import MiniHackDynamicMaze, MiniHack4Room\n",
    "from rl.dqn_train_loop import HRMAgentTrainingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0352eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities to visualise simple minihack room environment\n",
    "def tensor_to_string(chars_tensor):\n",
    "    np_chars = chars_tensor.cpu().numpy().transpose()\n",
    "    out = []\n",
    "    for row in range(np_chars.shape[0]):\n",
    "        string = \"\".join([chr(val) for val in np_chars[row, :]])\n",
    "        if string.strip() == \"\":\n",
    "            continue\n",
    "        out.append(string)\n",
    "    return out\n",
    "\n",
    "\n",
    "def pixels_to_img(pixels_tensor):\n",
    "    return Image.fromarray(pixels_tensor.numpy()[79:287, 608:816, :])\n",
    "\n",
    "\n",
    "# for navigation only\n",
    "def action_one_hot_to_string(action_one_hot_tensor):\n",
    "    np_action = action_one_hot_tensor.cpu().numpy()\n",
    "    np_action_idx = int(np_action.argmax())\n",
    "    dirs = {\n",
    "        0: \"north\",\n",
    "        1: \"east\",\n",
    "        2: \"south\",\n",
    "        3: \"west\",\n",
    "        4: \"north east\",\n",
    "        5: \"south east\",\n",
    "        6: \"south west\",\n",
    "        7: \"north west\",\n",
    "    }\n",
    "\n",
    "    return dirs[np_action_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43817f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get config\n",
    "from hydra import compose, initialize_config_dir\n",
    "from omegaconf import OmegaConf, SCMode\n",
    "\n",
    "with initialize_config_dir(\n",
    "    version_base=None,\n",
    "    config_dir=str(Path(\"../rl/config\").resolve()),\n",
    "    job_name=\"test_cfg\",\n",
    "):\n",
    "    cfg = compose(config_name=\"cfg_dqn.yaml\")\n",
    "\n",
    "typed_cfg: HRMQNetTrainingConfig = OmegaConf.to_container(\n",
    "    OmegaConf.merge(OmegaConf.structured(HRMQNetTrainingConfig), cfg),\n",
    "    structured_config_mode=SCMode.INSTANTIATE,\n",
    ")\n",
    "\n",
    "# for speed, we will reduce batch size, number of frames, etc.\n",
    "typed_cfg.dataset.env_kwargs[\"observation_keys\"] = [\"chars\"]\n",
    "# typed_cfg.dataset.env_name = \"MiniHack-4-Rooms\"\n",
    "typed_cfg.dataset.env_name = \"MiniHack-Corridor-Maze-4-Way-Dynamic\"\n",
    "typed_cfg.resume_from_run = None\n",
    "# typed_cfg.dataset.seq_len = 121\n",
    "typed_cfg.dataset.seq_len = 143\n",
    "typed_cfg.dataset.data_collection_batch_size = 32\n",
    "typed_cfg.dataset.frames_per_update = 320\n",
    "typed_cfg.log_wandb = False\n",
    "\n",
    "# set 4 or 8 way\n",
    "typed_cfg.dataset.action_space_size = 4\n",
    "typed_cfg.dataset.vocab_size = 131  # if 8, else 131\n",
    "typed_cfg.dataset.env_kwargs[\"action-space\"] = 4\n",
    "\n",
    "dataset = MiniHackFullObservationSimpleEnvironmentDataset(config=typed_cfg.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e30e28",
   "metadata": {},
   "source": [
    "# Test playing with 1 env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from copy import deepcopy\n",
    "\n",
    "dg_shape = (11, 13)\n",
    "\n",
    "env = dataset.create_env()\n",
    "inner_current_state = env.reset()\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # print last reward if any\n",
    "    if \"next\" in inner_current_state:\n",
    "        print(\n",
    "            \"Last action:\", action_one_hot_to_string(inner_current_state[0][\"action\"])\n",
    "        )\n",
    "        print(\"Last action reward:\", inner_current_state[0][\"reward\"].item())\n",
    "\n",
    "    # visualise the current environment\n",
    "    t = \"\\n\".join(tensor_to_string(inner_current_state[\"inputs\"].reshape(dg_shape).T))\n",
    "    print(t)\n",
    "\n",
    "    # Keyboard control (4 way)\n",
    "    x = input()\n",
    "\n",
    "    input_action_map = {\n",
    "        # Cardinals\n",
    "        \"w\": 0,  # move agent '@' north\n",
    "        \"a\": 3,  # west\n",
    "        \"s\": 2,  # south\n",
    "        \"d\": 1,  # east\n",
    "        # diagonals\n",
    "        \"k\": 4,  # NE\n",
    "        \"m\": 5,  # SE\n",
    "        \"n\": 6,  # SW\n",
    "        \"h\": 7,  # NW\n",
    "    }\n",
    "\n",
    "    # Cardinals\n",
    "    if x in input_action_map:\n",
    "        a = input_action_map[x]\n",
    "    else:\n",
    "        print(\"Breaking out of loop\")\n",
    "        break\n",
    "\n",
    "    # pass the current state through to our policy and get the actions to take\n",
    "    policy_decision = inner_current_state.clone()\n",
    "    policy_decision[\"action\"] = torch.tensor(\n",
    "        [0 for _ in range(typed_cfg.dataset.action_space_size)]\n",
    "    )\n",
    "    policy_decision[\"action\"][a] = 1\n",
    "\n",
    "    # step the environmet\n",
    "    transitions, inner_current_state = env.step_and_maybe_reset(policy_decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e18142",
   "metadata": {},
   "source": [
    "# Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2af7835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "dg_shape = (11, 13)\n",
    "\n",
    "test_cfg = deepcopy(typed_cfg)\n",
    "\n",
    "# interesting keys\n",
    "test_cfg.dataset.env_kwargs[MiniHackDynamicMaze.KEY_P_CHANGE_DOORS] = 0.25\n",
    "# test_cfg.dataset.env_kwargs[MiniHack4Room.KEY_MIN_ROOM_DISTANCE_BETWEEN_START_END] = 1  # does not apply for maze\n",
    "\n",
    "test_cfg.use_last_hidden_state_to_seed_next_environment_step = (\n",
    "    True  # could be true to test how things change\n",
    ")\n",
    "test_cfg.dataset.do_not_skip_running_model_if_random_action = (\n",
    "    test_cfg.use_last_hidden_state_to_seed_next_environment_step\n",
    ")\n",
    "\n",
    "if test_cfg.use_last_hidden_state_to_seed_next_environment_step:\n",
    "    test_cfg.dataset.training_batch_size = test_cfg.dataset.data_collection_batch_size\n",
    "\n",
    "test_dataset = MiniHackFullObservationSimpleEnvironmentDataset(config=test_cfg.dataset)\n",
    "env = test_dataset.create_env()\n",
    "with torch.device(\n",
    "    \"cuda\"\n",
    "):  # make sure that the buffers used in HRM are initialised on CUDA for backprop\n",
    "    hrm_agent_training_module = HRMAgentTrainingModule(test_cfg, test_dataset)\n",
    "\n",
    "# reconfigure the out keys of the modules so that things are kept\n",
    "if test_cfg.use_last_hidden_state_to_seed_next_environment_step:\n",
    "    hrm_agent_training_module.qvalue_net.config.use_last_hidden_state_to_seed_next_environment_step = test_cfg.use_last_hidden_state_to_seed_next_environment_step\n",
    "    hrm_agent_training_module.actor.out_keys += [\"seed_h_init\", \"seed_l_init\"]\n",
    "    hrm_agent_training_module.qvalue_net.out_keys += [\"seed_h_init\", \"seed_l_init\"]\n",
    "\n",
    "test_dataset.initialise_policy_and_collector(\n",
    "    hrm_agent_training_module.actor, hrm_agent_training_module.egreedy_module\n",
    ")\n",
    "hrm_agent_training_module.pre_training_setup(checkpoint_dir=\"s3\", run_name=\"exnjvjjo\")\n",
    "hrm_agent_training_module.load_from_checkpoint(\n",
    "    \"o2kstgjy\", ckpt_path_name=\"last.ckpt\", restore_config=False\n",
    ")\n",
    "\n",
    "model = hrm_agent_training_module.actor\n",
    "inner_current_state = env.reset()\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # print last reward if any\n",
    "    if \"next\" in inner_current_state:\n",
    "        print(\"Last action:\", action_one_hot_to_string(inner_current_state[\"action\"]))\n",
    "        print(\"Last action reward:\", inner_current_state[\"next\"][\"reward\"].item())\n",
    "\n",
    "    # visualise the current environment\n",
    "    t = \"\\n\".join(tensor_to_string(inner_current_state[\"inputs\"].reshape(dg_shape).T))\n",
    "    print(t)\n",
    "\n",
    "    # Keyboard control (4 way)\n",
    "    x = input(\"Continue?\")\n",
    "\n",
    "    # Cardinals\n",
    "    if (not x.lower().strip().startswith(\"y\")) and (not x == \"\"):\n",
    "        break\n",
    "\n",
    "    # pass the current state through to our policy and get the actions to take\n",
    "    policy_decision = inner_current_state.clone()\n",
    "\n",
    "    with torch.no_grad(), evaluating_net_context(model):\n",
    "        policy_decision = model(policy_decision.unsqueeze(0).cuda()).cpu()[0]\n",
    "\n",
    "    # step the environmet\n",
    "    inner_current_state = env.step(policy_decision)\n",
    "    inner_current_state[\"inputs\"] = inner_current_state[\"next\"][\"inputs\"]\n",
    "\n",
    "    if inner_current_state[\"next\"][\"terminated\"].item():\n",
    "        print(\"Episode ended. Resetting new environment.\")\n",
    "        env = test_dataset.create_env()\n",
    "        inner_current_state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea79b5",
   "metadata": {},
   "source": [
    "# Test validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc1e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_cfg = deepcopy(typed_cfg)\n",
    "\n",
    "# interesting keys\n",
    "test_cfg.dataset.env_kwargs[MiniHackDynamicMaze.KEY_P_CHANGE_DOORS] = 0.05\n",
    "# test_cfg.dataset.env_kwargs[MiniHack4Room.KEY_MIN_ROOM_DISTANCE_BETWEEN_START_END] = (\n",
    "#     None\n",
    "# )\n",
    "\n",
    "test_cfg.dataset.data_collection_batch_size = 1024\n",
    "test_cfg.dataset.frames_per_update = 1024\n",
    "test_cfg.use_last_hidden_state_to_seed_next_environment_step = (\n",
    "    True  # could be true to test how things change\n",
    ")\n",
    "test_cfg.dataset.do_not_skip_running_model_if_random_action = (\n",
    "    test_cfg.use_last_hidden_state_to_seed_next_environment_step\n",
    ")\n",
    "\n",
    "if test_cfg.use_last_hidden_state_to_seed_next_environment_step:\n",
    "    test_cfg.dataset.training_batch_size = test_cfg.dataset.data_collection_batch_size\n",
    "\n",
    "test_dataset = MiniHackFullObservationSimpleEnvironmentDataset(config=test_cfg.dataset)\n",
    "env = test_dataset.create_env()\n",
    "with torch.device(\n",
    "    \"cuda\"\n",
    "):  # make sure that the buffers used in HRM are initialised on CUDA for backprop\n",
    "    hrm_agent_training_module = HRMAgentTrainingModule(test_cfg, test_dataset)\n",
    "\n",
    "# reconfigure the out keys of the modules so that things are kept\n",
    "if test_cfg.use_last_hidden_state_to_seed_next_environment_step:\n",
    "    hrm_agent_training_module.qvalue_net.config.use_last_hidden_state_to_seed_next_environment_step = test_cfg.use_last_hidden_state_to_seed_next_environment_step\n",
    "    hrm_agent_training_module.actor.out_keys += [\"seed_h_init\", \"seed_l_init\"]\n",
    "    hrm_agent_training_module.qvalue_net.out_keys += [\"seed_h_init\", \"seed_l_init\"]\n",
    "\n",
    "test_dataset.initialise_policy_and_collector(\n",
    "    hrm_agent_training_module.actor, hrm_agent_training_module.egreedy_module\n",
    ")\n",
    "hrm_agent_training_module.pre_training_setup(checkpoint_dir=\"s3\", run_name=\"exnjvjjo\")\n",
    "hrm_agent_training_module.load_from_checkpoint(\n",
    "    \"o2kstgjy\",\n",
    "    ckpt_path_name=\"last.ckpt\",\n",
    "    restore_config=False,\n",
    ")\n",
    "\n",
    "validation_trajectories = test_dataset.validation_rollout()\n",
    "\n",
    "print(f\"% of episodes completed: {validation_trajectories['next']['reward'][:, -1, 0].mean().item():.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
