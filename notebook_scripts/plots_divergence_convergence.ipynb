{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60345dff-be7d-4514-8bb1-ebaf7782598f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"HRM Agent Plots write-up\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f441ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# | include: False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "import pickle\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "from copy import deepcopy\n",
    "from dotenv import load_dotenv\n",
    "from tensordict import TensorDict\n",
    "from torchrl.modules import QValueActor, EGreedyModule\n",
    "\n",
    "from rl.agent import HRMQNetTrainingConfig, HRMQValueNet\n",
    "from rl.dataset import MiniHackFullObservationSimpleEnvironmentDataset, GymRLDataset\n",
    "from rl.dqn_train_loop import HRMAgentTrainingModule\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec92b9-5688-4d89-93f3-43d0aa29c05c",
   "metadata": {},
   "source": [
    "We first define some useful utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b9b33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_cfg_env(cfg: HRMQNetTrainingConfig, env_name: str | None = None):\n",
    "    cfg_copy = deepcopy(cfg)\n",
    "\n",
    "    if env_name is not None:\n",
    "        assert (\n",
    "            env_name == \"MiniHack-Corridor-Maze-4-Way-Dynamic\"\n",
    "            or env_name == \"MiniHack-4-Rooms\"\n",
    "        )\n",
    "        if env_name == \"MiniHack-Corridor-Maze-4-Way-Dynamic\":\n",
    "            cfg_copy.dataset.env_name = env_name\n",
    "            cfg_copy.dataset.seq_len = 143\n",
    "        else:\n",
    "            cfg_copy.dataset.env_name = env_name\n",
    "            cfg_copy.dataset.seq_len = 121\n",
    "\n",
    "    return cfg_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767af100-5b08-43ba-8616-566613c95745",
   "metadata": {},
   "source": [
    "We first define a template environment with the following settings:\n",
    "\n",
    "- Probability of door change: 0.05\n",
    "- Number of environments used for evaluating model: 1024\n",
    "\n",
    "And other relevant settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0bbd6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get config\n",
    "from hydra import compose, initialize_config_dir\n",
    "from omegaconf import OmegaConf, SCMode\n",
    "\n",
    "with initialize_config_dir(\n",
    "    version_base=None,\n",
    "    config_dir=str(Path(\"../rl/config\").resolve()),\n",
    "    job_name=\"test_cfg\",\n",
    "):\n",
    "    cfg = compose(config_name=\"cfg_dqn.yaml\")\n",
    "\n",
    "typed_cfg: HRMQNetTrainingConfig = OmegaConf.to_container(\n",
    "    OmegaConf.merge(OmegaConf.structured(HRMQNetTrainingConfig), cfg),\n",
    "    structured_config_mode=SCMode.INSTANTIATE,\n",
    ")\n",
    "\n",
    "typed_cfg.dataset.env_kwargs[\"observation_keys\"] = [\"chars\"]\n",
    "typed_cfg.resume_from_run = None\n",
    "typed_cfg.dataset.data_collection_batch_size = 1024\n",
    "typed_cfg.dataset.frames_per_update = 1024\n",
    "typed_cfg.log_wandb = False\n",
    "\n",
    "# set 4 or 8 way\n",
    "typed_cfg.dataset.action_space_size = 4\n",
    "typed_cfg.dataset.vocab_size = 131\n",
    "typed_cfg.dataset.env_kwargs[\"action-space\"] = 4\n",
    "typed_cfg.dataset.env_kwargs[\"p-change-doors\"] = 0.05\n",
    "\n",
    "dataset = MiniHackFullObservationSimpleEnvironmentDataset(config=typed_cfg.dataset)\n",
    "\n",
    "dg_shape = (11, 11) if \"4-Room\" in typed_cfg.dataset.env_name else (11, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a31686",
   "metadata": {},
   "source": [
    "# Perform validation loop while reading off the last hidden over 8 steps\n",
    "\n",
    "We first build a class that allows us to attach hooks to the latent values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOAL_CHAR = \">\"\n",
    "\n",
    "\n",
    "class LoggingHRMQValueNet(HRMQValueNet):\n",
    "    def q_values_on_new_carry(self, batch_data):\n",
    "        \"\"\"\n",
    "        Compute Q-values for a given state and action.\n",
    "\n",
    "        This runs from an initial carry until the end of the sequence, which is useful for prediction. This has no gradient flow by default as backpropagation through time is untested in the HRM model.\n",
    "        \"\"\"\n",
    "\n",
    "        if intervention := getattr(self, \"intervention\", None):\n",
    "            if intervention == \"shift_target_randomly\":\n",
    "                batch_data = batch_data.clone()\n",
    "                empty_locs = batch_data[\"inputs\"] == ord(\".\")\n",
    "                has_empty_locs = (empty_locs == True).any(dim=1)\n",
    "                rand_scores = torch.rand_like(batch_data[\"inputs\"], dtype=torch.float)\n",
    "                rand_scores[~empty_locs] = float(\"-inf\")\n",
    "                batch_indices = torch.arange(\n",
    "                    batch_data.batch_size[0], device=batch_data.device\n",
    "                )[has_empty_locs]\n",
    "                selected_indices = rand_scores.argmax(dim=1)[has_empty_locs]\n",
    "                batch_data[\"inputs\"] = torch.where(\n",
    "                    batch_data[\"inputs\"] == ord(GOAL_CHAR),\n",
    "                    ord(\".\"),\n",
    "                    batch_data[\"inputs\"],\n",
    "                )\n",
    "                batch_data[\"inputs\"][batch_indices, selected_indices] = ord(GOAL_CHAR)\n",
    "\n",
    "        q_values = None\n",
    "        current_carry = self._initial_carry(batch_data).to(self.model.device)\n",
    "        is_first_carry = True  # first carry is all halted by design so the data is simply copied from batch data\n",
    "\n",
    "        running_z_H = [\n",
    "            self.model.inner.H_init.detach()\n",
    "            .unsqueeze(0)\n",
    "            .repeat(batch_data.batch_size[0], 1)\n",
    "        ]\n",
    "        running_z_L = [\n",
    "            self.model.inner.L_init.detach()\n",
    "            .unsqueeze(0)\n",
    "            .repeat(batch_data.batch_size[0], 1)\n",
    "        ]\n",
    "\n",
    "        while not current_carry.halted.all() or is_first_carry:\n",
    "            current_carry, q_values = self._q_values_with_carry(\n",
    "                carry=current_carry, batch_data=batch_data\n",
    "            )\n",
    "\n",
    "            # store the z_H and z_L so it doesn't get overwritten\n",
    "            running_z_H.append(current_carry.inner_carry.z_H[:, -1, :].detach())\n",
    "            running_z_L.append(current_carry.inner_carry.z_L[:, -1, :].detach())\n",
    "\n",
    "            is_first_carry = False\n",
    "\n",
    "            if intervention := getattr(self, \"intervention\", None):\n",
    "                if intervention == \"reset_converged_latents_to_init\":\n",
    "                    current_carry.inner_carry.z_H = (\n",
    "                        current_carry.inner_carry.z_H * 0\n",
    "                        + self.model.inner.H_init.unsqueeze(0).unsqueeze(0)\n",
    "                    )\n",
    "                    current_carry.inner_carry.z_L = (\n",
    "                        current_carry.inner_carry.z_L * 0\n",
    "                        + self.model.inner.L_init.unsqueeze(0).unsqueeze(0)\n",
    "                    )\n",
    "\n",
    "        if q_values is None:\n",
    "            raise ValueError(\"No q-values computed as initial carry were all halted\")\n",
    "\n",
    "        running_z_H = torch.stack(running_z_H, dim=1)  # (bs, 9, hd)\n",
    "        running_z_L = torch.stack(running_z_L, dim=1)  # (bs, 9, hd)\n",
    "        return current_carry, q_values, running_z_H, running_z_L\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        \"\"\"Forward starts from initial carry by default, used mostly during inference\"\"\"\n",
    "        if self.model.training:\n",
    "            raise ValueError(\"This should not be trained!\")\n",
    "\n",
    "        carry, action_values, running_z_H, running_z_L = self.q_values_on_new_carry(\n",
    "            batch_data\n",
    "        )\n",
    "        out_tensor = TensorDict(\n",
    "            {\n",
    "                **{k: v for k, v in batch_data.items() if k not in (\"action_value\",)},\n",
    "                \"action_value\": action_values,\n",
    "                \"running_z_H\": running_z_H,\n",
    "                \"running_z_L\": running_z_L,\n",
    "            },\n",
    "            batch_data.batch_size,\n",
    "            device=batch_data.device,\n",
    "        )\n",
    "\n",
    "        if self.config.use_last_hidden_state_to_seed_next_environment_step:\n",
    "            out_tensor[\"seed_h_init\"] = carry.inner_carry.z_H[:, -1, :].detach().clone()\n",
    "            out_tensor[\"seed_l_init\"] = carry.inner_carry.z_L[:, -1, :].detach().clone()\n",
    "\n",
    "        return out_tensor\n",
    "\n",
    "\n",
    "class LoggingHRMModule(HRMAgentTrainingModule):\n",
    "    def __init__(\n",
    "        self, config: HRMQNetTrainingConfig, dataset: GymRLDataset, model_class: object\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.qvalue_net = model_class(self.config)\n",
    "        self.dataset = dataset\n",
    "        self.actor = QValueActor(self.qvalue_net, spec=dataset.base_env.action_spec)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.egreedy_module = EGreedyModule(\n",
    "            spec=self.actor.spec,\n",
    "            annealing_num_steps=self.config.eps_decay_steps,\n",
    "            eps_init=self.config.start_eps,\n",
    "            eps_end=self.config.end_eps,\n",
    "        )\n",
    "\n",
    "        if self.config.use_target_network:\n",
    "            self.target_network = HRMQValueNet(config)\n",
    "            for param in self.target_network.parameters():\n",
    "                param.requires_grad = False\n",
    "            for buffer in self.target_network.buffers():\n",
    "                buffer.requires_grad = False\n",
    "            self.target_network.eval()\n",
    "\n",
    "        # validate some config\n",
    "        if self.config.use_last_hidden_state_to_seed_next_environment_step:\n",
    "            assert (\n",
    "                self.config.dataset.training_batch_size\n",
    "                == self.config.dataset.data_collection_batch_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1541978b-2793-4198-92c7-66cb6344d46e",
   "metadata": {},
   "source": [
    "We now collect the validation trajectories across different models and environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some metadata on the models we are using\n",
    "PICKLES_OUTPUT_DIR = Path(\".\").parent.parent / \"outputs\" / \"plots_trajectories_pickle\"\n",
    "PICKLES_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# each run takes around 6 minutes, so 2 hour for all\n",
    "ALL_RUNS = (\n",
    "    # vanilla\n",
    "    (\n",
    "        \"psy7bpwq\",\n",
    "        \"Reseeding 4 rooms\",\n",
    "        \"optim_step=193465_eval_metric=0.985.ckpt\",\n",
    "        True,\n",
    "        \"MiniHack-4-Rooms\",\n",
    "        None,\n",
    "    ),\n",
    "    (\n",
    "        \"psy7bpwq\",\n",
    "        \"Not reseeding 4 rooms\",\n",
    "        \"optim_step=193465_eval_metric=0.985.ckpt\",\n",
    "        False,\n",
    "        \"MiniHack-4-Rooms\",\n",
    "        None,\n",
    "    ),\n",
    "    (\"u2fivtrs\", \"Reseeding 4 rooms\", \"last.ckpt\", True, \"MiniHack-4-Rooms\", None),\n",
    "    (\"u2fivtrs\", \"Not reseeding 4 rooms\", \"last.ckpt\", False, \"MiniHack-4-Rooms\", None),\n",
    "    (\n",
    "        \"nq3r9hsw\",\n",
    "        \"Reseeding maze\",\n",
    "        \"optim_step=164261_eval_metric=0.973.ckpt\",\n",
    "        True,\n",
    "        \"MiniHack-Corridor-Maze-4-Way-Dynamic\",\n",
    "        None,\n",
    "    ),\n",
    "    (\n",
    "        \"nq3r9hsw\",\n",
    "        \"Not reseeding maze\",\n",
    "        \"optim_step=164261_eval_metric=0.973.ckpt\",\n",
    "        False,\n",
    "        \"MiniHack-Corridor-Maze-4-Way-Dynamic\",\n",
    "        None,\n",
    "    ),\n",
    "    (\n",
    "        \"f79zuqjx\",\n",
    "        \"Reseeding maze\",\n",
    "        \"last.ckpt\",\n",
    "        True,\n",
    "        \"MiniHack-Corridor-Maze-4-Way-Dynamic\",\n",
    "        None,\n",
    "    ),\n",
    "    (\n",
    "        \"f79zuqjx\",\n",
    "        \"Not reseeding maze\",\n",
    "        \"last.ckpt\",\n",
    "        False,\n",
    "        \"MiniHack-Corridor-Maze-4-Way-Dynamic\",\n",
    "        None,\n",
    "    ),\n",
    "    # with resetting intervention\n",
    "    (\n",
    "        \"psy7bpwq\",\n",
    "        \"Reseeding 4 rooms\",\n",
    "        \"optim_step=193465_eval_metric=0.985.ckpt\",\n",
    "        True,\n",
    "        \"MiniHack-4-Rooms\",\n",
    "        \"reset_converged_latents_to_init\",\n",
    "    ),\n",
    "    (\n",
    "        \"psy7bpwq\",\n",
    "        \"Not reseeding 4 rooms\",\n",
    "        \"optim_step=193465_eval_metric=0.985.ckpt\",\n",
    "        False,\n",
    "        \"MiniHack-4-Rooms\",\n",
    "        \"reset_converged_latents_to_init\",\n",
    "    ),\n",
    "    (\n",
    "        \"u2fivtrs\",\n",
    "        \"Reseeding 4 rooms\",\n",
    "        \"last.ckpt\",\n",
    "        True,\n",
    "        \"MiniHack-4-Rooms\",\n",
    "        \"reset_converged_latents_to_init\",\n",
    "    ),\n",
    "    (\n",
    "        \"u2fivtrs\",\n",
    "        \"Not reseeding 4 rooms\",\n",
    "        \"last.ckpt\",\n",
    "        False,\n",
    "        \"MiniHack-4-Rooms\",\n",
    "        \"reset_converged_latents_to_init\",\n",
    "    ),\n",
    "    (\n",
    "        \"nq3r9hsw\",\n",
    "        \"Reseeding maze\",\n",
    "        \"optim_step=164261_eval_metric=0.973.ckpt\",\n",
    "        True,\n",
    "        \"MiniHack-Corridor-Maze-4-Way-Dynamic\",\n",
    "        \"reset_converged_latents_to_init\",\n",
    "    ),\n",
    "    (\n",
    "        \"nq3r9hsw\",\n",
    "        \"Not reseeding maze\",\n",
    "        \"optim_step=164261_eval_metric=0.973.ckpt\",\n",
    "        False,\n",
    "        \"MiniHack-Corridor-Maze-4-Way-Dynamic\",\n",
    "        \"reset_converged_latents_to_init\",\n",
    "    ),\n",
    "    (\n",
    "        \"f79zuqjx\",\n",
    "        \"Reseeding maze\",\n",
    "        \"last.ckpt\",\n",
    "        True,\n",
    "        \"MiniHack-Corridor-Maze-4-Way-Dynamic\",\n",
    "        \"reset_converged_latents_to_init\",\n",
    "    ),\n",
    "    (\n",
    "        \"f79zuqjx\",\n",
    "        \"Not reseeding maze\",\n",
    "        \"last.ckpt\",\n",
    "        False,\n",
    "        \"MiniHack-Corridor-Maze-4-Way-Dynamic\",\n",
    "        \"reset_converged_latents_to_init\",\n",
    "    ),\n",
    "    # with random shift intervention\n",
    "    (\n",
    "        \"psy7bpwq\",\n",
    "        \"Reseeding 4 rooms\",\n",
    "        \"optim_step=193465_eval_metric=0.985.ckpt\",\n",
    "        True,\n",
    "        \"MiniHack-4-Rooms\",\n",
    "        \"shift_target_randomly\",\n",
    "    ),\n",
    "    (\n",
    "        \"psy7bpwq\",\n",
    "        \"Not reseeding 4 rooms\",\n",
    "        \"optim_step=193465_eval_metric=0.985.ckpt\",\n",
    "        False,\n",
    "        \"MiniHack-4-Rooms\",\n",
    "        \"shift_target_randomly\",\n",
    "    ),\n",
    "    (\n",
    "        \"u2fivtrs\",\n",
    "        \"Reseeding 4 rooms\",\n",
    "        \"last.ckpt\",\n",
    "        True,\n",
    "        \"MiniHack-4-Rooms\",\n",
    "        \"shift_target_randomly\",\n",
    "    ),\n",
    "    (\n",
    "        \"u2fivtrs\",\n",
    "        \"Not reseeding 4 rooms\",\n",
    "        \"last.ckpt\",\n",
    "        False,\n",
    "        \"MiniHack-4-Rooms\",\n",
    "        \"shift_target_randomly\",\n",
    "    ),\n",
    "    (\n",
    "        \"nq3r9hsw\",\n",
    "        \"Reseeding maze\",\n",
    "        \"optim_step=164261_eval_metric=0.973.ckpt\",\n",
    "        True,\n",
    "        \"MiniHack-Corridor-Maze-4-Way-Dynamic\",\n",
    "        \"shift_target_randomly\",\n",
    "    ),\n",
    "    (\n",
    "        \"nq3r9hsw\",\n",
    "        \"Not reseeding maze\",\n",
    "        \"optim_step=164261_eval_metric=0.973.ckpt\",\n",
    "        False,\n",
    "        \"MiniHack-Corridor-Maze-4-Way-Dynamic\",\n",
    "        \"shift_target_randomly\",\n",
    "    ),\n",
    "    (\n",
    "        \"f79zuqjx\",\n",
    "        \"Reseeding maze\",\n",
    "        \"last.ckpt\",\n",
    "        True,\n",
    "        \"MiniHack-Corridor-Maze-4-Way-Dynamic\",\n",
    "        \"shift_target_randomly\",\n",
    "    ),\n",
    "    (\n",
    "        \"f79zuqjx\",\n",
    "        \"Not reseeding maze\",\n",
    "        \"last.ckpt\",\n",
    "        False,\n",
    "        \"MiniHack-Corridor-Maze-4-Way-Dynamic\",\n",
    "        \"shift_target_randomly\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "for (\n",
    "    run_name,\n",
    "    desc,\n",
    "    ckpt_name,\n",
    "    set_hidden_state,\n",
    "    env_name,\n",
    "    intervention_desc,\n",
    ") in ALL_RUNS:\n",
    "    latents_pickle_file_name = (\n",
    "        f\"{run_name}_at_{ckpt_name}_reseeding={set_hidden_state}_env={env_name}_intervetion={intervention_desc if intervention_desc else 'None'}\"\n",
    "        + \".pkl\"\n",
    "    )\n",
    "    if (PICKLES_OUTPUT_DIR / latents_pickle_file_name).exists():\n",
    "        continue\n",
    "\n",
    "    test_cfg = set_cfg_env(typed_cfg, env_name)\n",
    "\n",
    "    test_cfg.use_last_hidden_state_to_seed_next_environment_step = set_hidden_state\n",
    "    if test_cfg.use_last_hidden_state_to_seed_next_environment_step:\n",
    "        test_cfg.dataset.training_batch_size = (\n",
    "            test_cfg.dataset.data_collection_batch_size\n",
    "        )\n",
    "    test_cfg.dataset.do_not_skip_running_model_if_random_action = (\n",
    "        test_cfg.use_last_hidden_state_to_seed_next_environment_step\n",
    "    )\n",
    "    test_dataset = MiniHackFullObservationSimpleEnvironmentDataset(\n",
    "        config=test_cfg.dataset\n",
    "    )\n",
    "\n",
    "    with torch.device(\n",
    "        \"cuda\"\n",
    "    ):  # make sure that the buffers used in HRM are initialised on CUDA for backprop\n",
    "        hrm_agent_training_module = LoggingHRMModule(\n",
    "            test_cfg, test_dataset, LoggingHRMQValueNet\n",
    "        )\n",
    "\n",
    "    if intervention_desc is not None:\n",
    "        hrm_agent_training_module.qvalue_net.intervention = intervention_desc\n",
    "\n",
    "    # reconfigure the out keys of the modules so that things are kept\n",
    "    if test_cfg.use_last_hidden_state_to_seed_next_environment_step:\n",
    "        hrm_agent_training_module.qvalue_net.config.use_last_hidden_state_to_seed_next_environment_step = test_cfg.use_last_hidden_state_to_seed_next_environment_step\n",
    "        hrm_agent_training_module.actor.out_keys += [\"seed_h_init\", \"seed_l_init\"]\n",
    "        hrm_agent_training_module.qvalue_net.out_keys += [\"seed_h_init\", \"seed_l_init\"]\n",
    "\n",
    "    hrm_agent_training_module.qvalue_net.out_keys += [\"running_z_H\", \"running_z_L\"]\n",
    "    hrm_agent_training_module.actor.out_keys += [\"running_z_H\", \"running_z_L\"]\n",
    "\n",
    "    test_dataset.initialise_policy_and_collector(hrm_agent_training_module.actor, None)\n",
    "    hrm_agent_training_module.pre_training_setup(checkpoint_dir=\"s3\", run_name=run_name)\n",
    "    hrm_agent_training_module.load_from_checkpoint(\n",
    "        run_name, ckpt_path_name=ckpt_name, restore_config=False\n",
    "    )\n",
    "\n",
    "    validation_trajectories = test_dataset.validation_rollout()\n",
    "\n",
    "    # pickle the output for speedup\n",
    "    with open(PICKLES_OUTPUT_DIR / latents_pickle_file_name, \"wb\") as f:\n",
    "        pickle.dump(validation_trajectories, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a2e067",
   "metadata": {},
   "source": [
    "We also define utilities for converting the trajectories into metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c29fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from typing import Callable\n",
    "from pathlib import Path\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from utils.a_star import a_star\n",
    "\n",
    "\n",
    "def get_convergence_divergence_metrics(\n",
    "    trajectories: TensorDict,\n",
    "    dg_shape: tuple[int, int],\n",
    "    latent_key: str = \"running_z_H\",\n",
    "    against_env_init_latents=False,\n",
    "    against_seed_latents=False,\n",
    "    against_first_latents=False,\n",
    "):\n",
    "    ignore_repeat_after_termination_mask = torch.logical_and(\n",
    "        trajectories[\"next\"][\"reward\"][:, :, 0] == 1,\n",
    "        torch.roll(trajectories[\"next\"][\"reward\"][:, :, 0], shifts=1, dims=1) == 1,\n",
    "    )\n",
    "    ignore_repeat_after_termination_mask[:, 0] = (\n",
    "        True  # Mask the first as it action is kinda special since the model always start from init, We want to chart convergence speed given different inits and environment change flag\n",
    "    )\n",
    "\n",
    "    latent_values = trajectories[latent_key]  # (bs, num_env_steps, recursive_steps, hd)\n",
    "\n",
    "    if against_env_init_latents:\n",
    "        assert not against_seed_latents and not against_first_latents\n",
    "        comparison_latents = (\n",
    "            trajectories[latent_key][:, 0, 0, :].unsqueeze(1).unsqueeze(1)\n",
    "        )  # (bs, 1, 1, hd)\n",
    "    elif against_seed_latents:\n",
    "        assert not against_first_latents\n",
    "        comparison_latents = trajectories[latent_key][:, :, 0, :].unsqueeze(\n",
    "            2\n",
    "        )  # (bs, num_env_steps, 1, hd)\n",
    "    elif against_first_latents:\n",
    "        comparison_latents = trajectories[latent_key][:, :, 1, :].unsqueeze(\n",
    "            2\n",
    "        )  # (bs, num_env_steps, 1, hd)\n",
    "    else:\n",
    "        comparison_latents = trajectories[latent_key][:, :, -1, :].unsqueeze(\n",
    "            2\n",
    "        )  # (bs, num_env_steps, 1, hd)\n",
    "\n",
    "    diff_mse = ((comparison_latents - latent_values) ** 2).sum(\n",
    "        dim=-1\n",
    "    ) / latent_values.shape[-1]  # (bs, num_env_steps, recursive_steps)\n",
    "\n",
    "    last_reward = torch.roll(\n",
    "        trajectories[\"next\"][\"reward\"][:, :, 0], shifts=1, dims=-1\n",
    "    )  # (bs, num_env_steps)\n",
    "    mask = last_reward == 1  # (bs, num_env_steps)\n",
    "    mask[:, 0] = True  # ignore first action\n",
    "\n",
    "    # get change flag for each env_step\n",
    "    change_from_prev_flags = [\n",
    "        torch.as_tensor([True] * latent_values.shape[0], dtype=torch.bool)\n",
    "    ]  # first step always add the first door\n",
    "    for env_step in range(1, latent_values.shape[1]):\n",
    "        # get change flag for this env step\n",
    "        inputs = trajectories[\"inputs\"][:, env_step, :]  # (bs, seq_len)\n",
    "        last_inputs = trajectories[\"inputs\"][:, env_step - 1, :]  # (bs, seq_len)\n",
    "        inputs_ignored_actor_start = torch.where(\n",
    "            torch.logical_or(inputs == ord(\"@\"), inputs == ord(\"<\")), ord(\".\"), inputs\n",
    "        )  # mask out the actor and start goal so the only change is in the doors of the env\n",
    "        last_inputs_ignored_actor_start = torch.where(\n",
    "            torch.logical_or(last_inputs == ord(\"@\"), last_inputs == ord(\"<\")),\n",
    "            ord(\".\"),\n",
    "            last_inputs,\n",
    "        )  # mask out the actor and start goal so the only change is in the doors of the env\n",
    "        env_change_at_all = (\n",
    "            last_inputs_ignored_actor_start != inputs_ignored_actor_start\n",
    "        ).any(dim=-1)  # (bs)\n",
    "\n",
    "        change_from_prev_flags.append([])\n",
    "        for env_idx in range(latent_values.shape[0]):\n",
    "            # if env didn't change at all, skip\n",
    "            if not env_change_at_all[env_idx]:\n",
    "                change_from_prev_flags[-1].append(False)\n",
    "                continue\n",
    "\n",
    "            # reconstruct the env_idx\n",
    "            prev_input_at_env_idx = last_inputs[env_idx, :].reshape(dg_shape).T\n",
    "            prev_agent_pos = list(\n",
    "                map(\n",
    "                    int, torch.nonzero(prev_input_at_env_idx == ord(\"@\"), as_tuple=True)\n",
    "                )\n",
    "            )[::-1]\n",
    "            prev_goal_pos = list(\n",
    "                map(\n",
    "                    int, torch.nonzero(prev_input_at_env_idx == ord(\">\"), as_tuple=True)\n",
    "                )\n",
    "            )[::-1]\n",
    "            changed_locations = torch.nonzero(\n",
    "                last_inputs_ignored_actor_start[env_idx, :].reshape(dg_shape).T\n",
    "                != inputs_ignored_actor_start[env_idx, :].reshape(dg_shape).T,\n",
    "                as_tuple=True,\n",
    "            )\n",
    "\n",
    "            # do a-star\n",
    "            prev_input_at_env_idx_str = [\n",
    "                [chr(val) if chr(val) not in (\"|\", \"-\", \"#\") else \"|\" for val in row]\n",
    "                for row in prev_input_at_env_idx.T\n",
    "            ]\n",
    "            path_from_agent_to_goal = a_star(\n",
    "                prev_input_at_env_idx_str,\n",
    "                start=prev_agent_pos,\n",
    "                goal=prev_goal_pos,\n",
    "                obstacle=\"|\",\n",
    "            )\n",
    "\n",
    "            # check that the door being updated is not on the path\n",
    "            if path_from_agent_to_goal is not None:\n",
    "                change_flag = False\n",
    "                for c, r in zip(*changed_locations):\n",
    "                    if [int(r), int(c)] in path_from_agent_to_goal[\n",
    "                        1:\n",
    "                    ]:  # skip the first one since it includes the agent_pos\n",
    "                        change_flag = True\n",
    "                        break\n",
    "\n",
    "            else:  # path only change if there is path now\n",
    "                current_input_at_env_idx = inputs[env_idx, :].reshape(dg_shape)\n",
    "                current_agent_pos = list(\n",
    "                    map(\n",
    "                        int,\n",
    "                        torch.nonzero(\n",
    "                            current_input_at_env_idx == ord(\"@\"), as_tuple=True\n",
    "                        ),\n",
    "                    )\n",
    "                )[::-1]\n",
    "                current_input_at_env_idx_str = [\n",
    "                    [\n",
    "                        chr(val) if chr(val) not in (\"|\", \"-\", \"#\") else \"|\"\n",
    "                        for val in row\n",
    "                    ]\n",
    "                    for row in current_input_at_env_idx.T\n",
    "                ]\n",
    "                change_flag = (\n",
    "                    a_star(\n",
    "                        current_input_at_env_idx_str,\n",
    "                        start=current_agent_pos,\n",
    "                        goal=prev_goal_pos,\n",
    "                        obstacle=\"|\",\n",
    "                    )\n",
    "                    is not None\n",
    "                )\n",
    "\n",
    "            change_from_prev_flags[-1].append(change_flag)\n",
    "\n",
    "        change_from_prev_flags[-1] = torch.as_tensor(\n",
    "            change_from_prev_flags[-1], dtype=torch.bool\n",
    "        )  # (bs)\n",
    "    change_from_prev_flags = torch.stack(\n",
    "        change_from_prev_flags, dim=1\n",
    "    )  # (bs, num_env_steps)\n",
    "\n",
    "    # change_from_prev_flags = change_from_p?rev_flags != 2 # all true\n",
    "\n",
    "    # we want to know for each recursive step and change flag, what the avg mse is and what the size of the support\n",
    "    mask_change_flag_true = (~mask) * change_from_prev_flags  # (bs, num_env_steps)\n",
    "    mask_change_flag_false = (~mask) * (~change_from_prev_flags)  # (bs, num_env_steps)\n",
    "\n",
    "    masked_change_true_diff_mse = torch.masked_select(\n",
    "        diff_mse, mask_change_flag_true.unsqueeze(2)\n",
    "    )  # (bs, num_env_steps, recursive_steps)\n",
    "    support_mask_change_flag_true = mask_change_flag_true.sum()  # scalar\n",
    "    mse_avg_recursive_step__change_flag_true = (\n",
    "        masked_change_true_diff_mse.sum() / support_mask_change_flag_true\n",
    "    )  # scalar\n",
    "\n",
    "    masked_change_false_diff_mse = torch.masked_select(\n",
    "        diff_mse, mask_change_flag_false.unsqueeze(2)\n",
    "    )  # (bs, num_env_steps, recursive_steps)\n",
    "    support_mask_change_flag_false = mask_change_flag_false.sum()  # scalar\n",
    "    mse_avg_recursive_step__change_flag_false = (\n",
    "        masked_change_false_diff_mse.sum() / support_mask_change_flag_false\n",
    "    )  # (recursive step)\n",
    "\n",
    "    return {\n",
    "        \"mse_avg_change_env\": mse_avg_recursive_step__change_flag_true,  # recursive step values\n",
    "        \"change_env_support\": support_mask_change_flag_true,\n",
    "        \"mse_avg_no_change_env\": mse_avg_recursive_step__change_flag_false,  # recursive step values\n",
    "        \"no_change_env_support\": support_mask_change_flag_false,\n",
    "        \"change_flags\": change_from_prev_flags,\n",
    "        \"diff_mse\": diff_mse,\n",
    "        \"mask_change_flag_true\": mask_change_flag_true,\n",
    "        \"mask_change_flag_false\": mask_change_flag_false,\n",
    "        \"num_envs_solved\": int((trajectories[\"next\"][\"reward\"][:, -1, 0] == 1).sum()),\n",
    "        \"num_envs_totaled\": trajectories.shape[0],\n",
    "    }\n",
    "\n",
    "\n",
    "def find_all_runs_matching_desc(\n",
    "    plots_dir: Path,\n",
    "    reseeding_flag: bool = True,\n",
    "    env_name: str = \"MiniHack-4-Rooms\",\n",
    "    intervention: str = \"None\",\n",
    "):\n",
    "    for pkl_file in plots_dir.glob(\"*.pkl\"):\n",
    "        if pkl_file.name.endswith(\n",
    "            f\"_reseeding={reseeding_flag}_env={env_name}_intervetion={intervention}.pkl\"\n",
    "        ):  # oopse misspelt\n",
    "            yield pkl_file\n",
    "\n",
    "\n",
    "def plot_convergence_divergence(\n",
    "    env_name: str,\n",
    "    intervention: str,\n",
    "    dg_shape: tuple[int, int],\n",
    "    pickles_dir: Path,\n",
    "    plots_dir: Path | None = None,\n",
    "    against_env_init_latents: bool = False,\n",
    "    against_seed_latents: bool = False,\n",
    "    against_first_latents: bool = False,\n",
    "    title: str | None = None,\n",
    "    y_label: str = \"Median MSE\",\n",
    "    plot_func: Callable[[torch.Tensor], torch.Tensor] = lambda y_tensor: torch.quantile(\n",
    "        y_tensor, q=torch.tensor([0.5]), dim=0\n",
    "    ).squeeze(),\n",
    "    y_scale: str = \"linear\",\n",
    "):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 8), constrained_layout=True)\n",
    "\n",
    "    if plots_dir is None:\n",
    "        plots_dir = pickles_dir.parent / \"plots_pngs\"\n",
    "        plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for plot_idx, key in enumerate((\"running_z_L\", \"running_z_H\")):\n",
    "        ax = axs[plot_idx]\n",
    "        ax.set_title(key.replace(\"running_\", \"\"), fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "        data_records = []\n",
    "        metadata_records = {}\n",
    "\n",
    "        dash_styles = {}\n",
    "\n",
    "        # Aggregate all data into a long-format list for seaborn\n",
    "        for reseeding_flag in (True, False):\n",
    "            measurement_metrics = []\n",
    "\n",
    "            for pkl_file in find_all_runs_matching_desc(\n",
    "                plots_dir=pickles_dir,\n",
    "                reseeding_flag=reseeding_flag,\n",
    "                env_name=env_name,\n",
    "                intervention=intervention,\n",
    "            ):\n",
    "                print(f\"Reading pickle {pkl_file}\")\n",
    "                with open(pkl_file, \"rb\") as f:\n",
    "                    trajectories = pickle.load(f)\n",
    "\n",
    "                measurement_metrics.append(\n",
    "                    get_convergence_divergence_metrics(\n",
    "                        trajectories=trajectories,\n",
    "                        dg_shape=dg_shape,\n",
    "                        latent_key=key,\n",
    "                        against_env_init_latents=against_env_init_latents,\n",
    "                        against_seed_latents=against_seed_latents,\n",
    "                        against_first_latents=against_first_latents,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            for change_flag in (True, False):\n",
    "                mask_key = f\"mask_change_flag_{change_flag}\".lower()\n",
    "\n",
    "                X_ALL = list(range(measurement_metrics[0][\"diff_mse\"].shape[-1]))\n",
    "                Y_ALL = [[] for _ in range(len(X_ALL))]\n",
    "\n",
    "                for m in measurement_metrics:\n",
    "                    Y_tensor = m[\"diff_mse\"]\n",
    "                    MASK = m[mask_key]\n",
    "                    for step in range(Y_tensor.shape[-1]):\n",
    "                        Y_ALL[step].extend(\n",
    "                            torch.masked_select(Y_tensor[:, :, step], MASK).float()\n",
    "                        )\n",
    "\n",
    "                Y_ALL = torch.tensor(Y_ALL)  # (recursion_steps, N_ALL)\n",
    "                plot_y = plot_func(torch.swapaxes(Y_ALL, 0, 1))\n",
    "\n",
    "                if y_scale == \"log\":\n",
    "                    plot_y = torch.maximum(\n",
    "                        plot_y, torch.tensor(torch.finfo(torch.float32).eps)\n",
    "                    )\n",
    "\n",
    "                label = (\n",
    "                    (\"Carry Z\" if reseeding_flag else \"Reset Z\")\n",
    "                    + \" - \"\n",
    "                    + (\"Env. changed\" if change_flag else \"Env. static\")\n",
    "                    + f\" (N={Y_ALL.shape[1]})\"\n",
    "                )\n",
    "\n",
    "                dash_styles[label] = \"\" if reseeding_flag else (2, 2)\n",
    "\n",
    "                for x, y in zip(X_ALL[1:-1], plot_y[1:-1].numpy()):\n",
    "                    data_records.append(\n",
    "                        {\"Recurrent step\": x, \"Median MSE\": y, \"Condition\": label}\n",
    "                    )\n",
    "\n",
    "            metadata_records[reseeding_flag] = {\n",
    "                \"num_envs\": sum(m[\"num_envs_totaled\"] for m in measurement_metrics),\n",
    "                \"num_envs_solved\": sum(\n",
    "                    m[\"num_envs_solved\"] for m in measurement_metrics\n",
    "                ),\n",
    "            }\n",
    "            metadata_records[reseeding_flag][\"frac_solved\"] = (\n",
    "                metadata_records[reseeding_flag][\"num_envs_solved\"]\n",
    "                / metadata_records[reseeding_flag][\"num_envs\"]\n",
    "            )\n",
    "\n",
    "        # --- Plot with seaborn ---\n",
    "        sns.lineplot(\n",
    "            data=pd.DataFrame(data_records),\n",
    "            x=\"Recurrent step\",\n",
    "            y=\"Median MSE\",\n",
    "            hue=\"Condition\",\n",
    "            ax=ax,\n",
    "            style=\"Condition\",\n",
    "            dashes=dash_styles,\n",
    "            palette=\"colorblind\",\n",
    "            linewidth=2,\n",
    "            legend=plot_idx == len(axs) - 1,\n",
    "        )\n",
    "\n",
    "        if plot_idx == 0:\n",
    "            ax.set_ylabel(\"Median MSE\" if y_label is None else y_label, fontsize=14)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\", fontsize=14)\n",
    "\n",
    "        ax.set_xlabel(\"Recurrent step\", fontsize=16)\n",
    "\n",
    "        ax.set_yscale(y_scale)\n",
    "\n",
    "        if plot_idx == len(axs) - 1:\n",
    "            meta_legend = [\n",
    "                Line2D(\n",
    "                    [],\n",
    "                    [],\n",
    "                    color=\"none\",\n",
    "                    label=f\"Carry Z: {metadata_records[True]['frac_solved'] * 100:.1f}%\",\n",
    "                ),\n",
    "                Line2D(\n",
    "                    [],\n",
    "                    [],\n",
    "                    color=\"none\",\n",
    "                    label=f\"Reset Z: {metadata_records[False]['frac_solved'] * 100:.1f}%\",\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "            # Add the secondary metadata legend aligned left with the first\n",
    "            info_legend = ax.legend(\n",
    "                handles=meta_legend,\n",
    "                loc=\"upper left\",\n",
    "                bbox_to_anchor=(1.02, 0.75),  # same x (1.02), slightly lower y\n",
    "                frameon=False,\n",
    "                title=f\"Environments solved (N={sum(int(d['num_envs']) for d in metadata_records.values()) // 2}):\",  # reseeding and non-reseeding\n",
    "                title_fontsize=16,\n",
    "                fontsize=16,\n",
    "            )\n",
    "\n",
    "            # add the main legend as it is longer and we want it used to define the width of the figure\n",
    "            ax.legend(\n",
    "                title=None,\n",
    "                frameon=False,\n",
    "                loc=\"upper left\",\n",
    "                bbox_to_anchor=(1.02, 1),\n",
    "                fontsize=16,\n",
    "            )\n",
    "\n",
    "            # Ensure the first legend remains visible (since adding a new one replaces the previous)\n",
    "            ax.add_artist(info_legend)\n",
    "\n",
    "    if title is None:\n",
    "        title = (\n",
    "            f\"Convergence of hidden states for {X_ALL[-1]} recurrent steps - {env_name}\"\n",
    "        )\n",
    "    fig.suptitle(title, fontsize=18)  # , fontweight=\"bold\")\n",
    "\n",
    "    file_name = re.sub(r\"[^a-zA-Z0-9]\", \"_\", title.lower())[:175] + \".png\"\n",
    "    plt.savefig(\n",
    "        plots_dir / file_name,\n",
    "        dpi=300,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d15c6",
   "metadata": {},
   "source": [
    "# Divergence and convergence plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24918014",
   "metadata": {},
   "source": [
    "Paper results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e5d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 11)\n",
    "env_name = \"MiniHack-4-Rooms\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"None\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    title=\"Convergence of recurrent state $z$ to final values - 4-rooms environment\",\n",
    "    y_scale=\"log\",\n",
    "    # y_scale=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eeb8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 13)\n",
    "env_name = \"MiniHack-Corridor-Maze-4-Way-Dynamic\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"None\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    title=\"Convergence of recurrent state $z$ to final values: Maze environment\",\n",
    "    y_scale=\"log\",\n",
    "    # y_scale=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 13)\n",
    "env_name = \"MiniHack-Corridor-Maze-4-Way-Dynamic\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"None\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    title=\"Divergence of recurrent state $z$ from initial values: Maze environment\",\n",
    "    against_first_latents=True,\n",
    "    y_scale=\"log\",\n",
    "    # y_scale=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879d54c",
   "metadata": {},
   "source": [
    "General plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30153799",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 11)\n",
    "env_name = \"MiniHack-4-Rooms\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"None\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    title=\"Convergence of MSE against final converged hidden states - 4-Rooms environment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a29c3-ecf3-4358-b7ae-5d0284f299aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 11)\n",
    "env_name = \"MiniHack-4-Rooms\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"None\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    y_label=\"STD MSE\",\n",
    "    y_scale=\"log\",\n",
    "    plot_func=lambda y: torch.std(y, dim=0).squeeze(),\n",
    "    title=\"Standard deviation in MSE against final converged hidden states - 4-Rooms environment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30576538",
   "metadata": {},
   "source": [
    "For divergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 11)\n",
    "env_name = \"MiniHack-4-Rooms\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"None\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    against_seed_latents=True,\n",
    "    title=\"Divergence of hidden states from initial hidden state used to seed the model - 4-Rooms environment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d853fb-534c-4527-a62a-da7156b24de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 11)\n",
    "env_name = \"MiniHack-4-Rooms\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"None\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    against_first_latents=True,\n",
    "    y_label=\"STD MSE\",\n",
    "    y_scale=\"log\",\n",
    "    plot_func=lambda y: torch.std(y, dim=0).squeeze(),\n",
    "    title=\"Standard deviation in MSE against 1-step hidden states - 4-Rooms environment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3bc463-ed78-4e1a-a2ca-724d947c3e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 11)\n",
    "env_name = \"MiniHack-4-Rooms\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"None\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    against_first_latents=True,\n",
    "    title=\"Divergence of MSE of subsequent recurrent steps against 1-step hidden states - 4-Rooms environment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba31610",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 11)\n",
    "env_name = \"MiniHack-4-Rooms\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"reset_converged_latents_to_init\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    against_first_latents=True,\n",
    "    title=\"MSE of subsequent recurrent steps against 1-step hidden states - 4-Rooms environment\\nIntervention: hidden states are reset to initial states after every step\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40df17d-6ef1-49ec-a1f8-7b5adf1193e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 11)\n",
    "env_name = \"MiniHack-4-Rooms\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"reset_converged_latents_to_init\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    against_env_init_latents=True,\n",
    "    title=\"Divergence of MSE of against hidden states used to init the environment - 4-Rooms environment\\nIntervention: hidden states are reset to initial states after every step\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9605129",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 11)\n",
    "env_name = \"MiniHack-4-Rooms\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"shift_target_randomly\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    title=\"Convergence of MSE against final converged hidden states - 4-Rooms environment\\nIntervention: model input is patched so the goal is resetted at a random empty location\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed94ecd",
   "metadata": {},
   "source": [
    "# Dynamic maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547acf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 13)\n",
    "env_name = \"MiniHack-Corridor-Maze-4-Way-Dynamic\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"None\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    title=\"Convergence of MSE against final converged hidden states - Maze environment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672da802-2eec-43f9-b697-5651a33e31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 13)\n",
    "env_name = \"MiniHack-Corridor-Maze-4-Way-Dynamic\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"None\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    y_label=\"STD MSE\",\n",
    "    y_scale=\"log\",\n",
    "    plot_func=lambda y: torch.std(y, dim=0).squeeze(),\n",
    "    title=\"Standard deviation in MSE against final converged hidden states - Maze environment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa6b58c",
   "metadata": {},
   "source": [
    "For divergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 13)\n",
    "env_name = \"MiniHack-Corridor-Maze-4-Way-Dynamic\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"None\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    against_seed_latents=True,\n",
    "    title=\"Divergence of hidden states from initial hidden state used to seed the model - Maze environment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 13)\n",
    "env_name = \"MiniHack-Corridor-Maze-4-Way-Dynamic\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"None\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    against_first_latents=True,\n",
    "    title=\"Divergence of MSE of subsequent recurrent steps against 1-step hidden states - Maze environment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a74a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 13)\n",
    "env_name = \"MiniHack-Corridor-Maze-4-Way-Dynamic\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"reset_converged_latents_to_init\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    against_env_init_latents=True,\n",
    "    title=\"Divergence of MSE of against hidden states used to init the environment - Maze environment\\nIntervention: hidden states are reset to initial states after every step\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eb70b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dg_shape = (11, 13)\n",
    "env_name = \"MiniHack-Corridor-Maze-4-Way-Dynamic\"\n",
    "\n",
    "plot_convergence_divergence(\n",
    "    dg_shape=dg_shape,\n",
    "    env_name=env_name,\n",
    "    intervention=\"shift_target_randomly\",\n",
    "    pickles_dir=PICKLES_OUTPUT_DIR,\n",
    "    title=\"Convergence of MSE against final converged hidden states - Maze environment\\nIntervention: model input is patched so the goal is resetted at a random empty location\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
