{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchrl.envs import SerialEnv\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyTensorStorage\n",
    "\n",
    "from rl.agent import (\n",
    "    HRMQNetTrainingConfig,\n",
    ")\n",
    "from rl.dataset import MiniHackFullObservationSimpleEnvironmentDataset\n",
    "from rl.dqn_train_loop import HRMAgentTrainingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0352eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_action_map = {\n",
    "    # Cardinals\n",
    "    \"w\": 0,  # move agent '@' north\n",
    "    \"a\": 3,  # west\n",
    "    \"s\": 2,  # south\n",
    "    \"d\": 1,  # east\n",
    "    # diagonals\n",
    "    \"k\": 4,  # NE\n",
    "    \"m\": 5,  # SE\n",
    "    \"n\": 6,  # SW\n",
    "    \"h\": 7,  # NW\n",
    "}\n",
    "\n",
    "\n",
    "# utilities to visualise simple minihack room environment\n",
    "def tensor_to_string(chars_tensor):\n",
    "    np_chars = chars_tensor.cpu().numpy().transpose()\n",
    "    out = []\n",
    "    for row in range(np_chars.shape[0]):\n",
    "        string = \"\".join([chr(val) for val in np_chars[row, :]])\n",
    "        if string.strip() == \"\":\n",
    "            continue\n",
    "        out.append(string)\n",
    "    return out\n",
    "\n",
    "\n",
    "# for navigation only\n",
    "def action_one_hot_to_string(action_one_hot_tensor):\n",
    "    np_action = action_one_hot_tensor.cpu().numpy()\n",
    "    np_action_idx = int(np_action.argmax())\n",
    "    dirs = {\n",
    "        0: \"north\",\n",
    "        1: \"east\",\n",
    "        2: \"south\",\n",
    "        3: \"west\",\n",
    "        4: \"north east\",\n",
    "        5: \"south east\",\n",
    "        6: \"south west\",\n",
    "        7: \"north west\",\n",
    "    }\n",
    "\n",
    "    return dirs[np_action_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43817f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get config\n",
    "from hydra import compose, initialize_config_dir\n",
    "from omegaconf import OmegaConf, SCMode\n",
    "\n",
    "with initialize_config_dir(\n",
    "    version_base=None,\n",
    "    config_dir=str(Path(\"../rl/config\").resolve()),\n",
    "    job_name=\"test_cfg\",\n",
    "):\n",
    "    cfg = compose(config_name=\"cfg_dqn.yaml\")\n",
    "\n",
    "typed_cfg: HRMQNetTrainingConfig = OmegaConf.to_container(\n",
    "    OmegaConf.merge(OmegaConf.structured(HRMQNetTrainingConfig), cfg),\n",
    "    structured_config_mode=SCMode.INSTANTIATE,\n",
    ")\n",
    "\n",
    "# for speed, we will reduce batch size, number of frames, etc.\n",
    "typed_cfg.dataset.env_name = \"MiniHack-Corridor-Maze-4-Way-Dynamic\"\n",
    "typed_cfg.dataset.seq_len = 13 * 11\n",
    "typed_cfg.dataset.data_collection_batch_size = 32\n",
    "typed_cfg.dataset.frames_per_update = 320\n",
    "\n",
    "# for testing only\n",
    "typed_cfg.use_last_hidden_state_to_seed_next_environment_step = True\n",
    "typed_cfg.dataset.do_not_skip_running_model_if_random_action = (\n",
    "    typed_cfg.use_last_hidden_state_to_seed_next_environment_step\n",
    ")\n",
    "if typed_cfg.use_last_hidden_state_to_seed_next_environment_step:\n",
    "   typed_cfg.dataset.training_batch_size = typed_cfg.dataset.data_collection_batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MiniHackFullObservationSimpleEnvironmentDataset(config=typed_cfg.dataset)\n",
    "with torch.device(\n",
    "    \"cuda\"\n",
    "):  # make sure that the buffers used in HRM are initialised on CUDA for backprop\n",
    "    hrm_agent_training_module = HRMAgentTrainingModule(typed_cfg, dataset)\n",
    "dataset.initialise_policy_and_collector(\n",
    "    hrm_agent_training_module.actor, hrm_agent_training_module.egreedy_module\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e30e28",
   "metadata": {},
   "source": [
    "# Test playing with 1 env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from copy import deepcopy\n",
    "\n",
    "new_cfg = deepcopy(typed_cfg)\n",
    "new_cfg.dataset.data_collection_batch_size = 1\n",
    "\n",
    "dg_shape = (11, 13)\n",
    "\n",
    "dataset_play = MiniHackFullObservationSimpleEnvironmentDataset(config=new_cfg.dataset)\n",
    "envs = SerialEnv(dataset_play.config.frames_per_update, dataset_play.create_env)\n",
    "inner_current_state = envs.reset()\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # print last reward if any\n",
    "    if \"next\" in inner_current_state:\n",
    "        print(\n",
    "            \"Last action:\", action_one_hot_to_string(inner_current_state[0][\"action\"])\n",
    "        )\n",
    "        print(\"Last action reward:\", inner_current_state[0][\"reward\"].item())\n",
    "\n",
    "    # visualise the current environment\n",
    "    t = tensor_to_string(inner_current_state[0][\"inputs\"].reshape(dg_shape).T)\n",
    "    print(\"\\n\".join(t))\n",
    "\n",
    "    # Keyboard control (4 way)\n",
    "    x = input()\n",
    "\n",
    "    input_action_map = {\n",
    "        # Cardinals\n",
    "        \"w\": 0,  # move agent '@' north\n",
    "        \"a\": 3,  # west\n",
    "        \"s\": 2,  # south\n",
    "        \"d\": 1,  # east\n",
    "        # diagonals\n",
    "        \"k\": 4,  # NE\n",
    "        \"m\": 5,  # SE\n",
    "        \"n\": 6,  # SW\n",
    "        \"h\": 7,  # NW\n",
    "    }\n",
    "\n",
    "    if x in input_action_map:\n",
    "        a = input_action_map[x]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    # pass the current state through to our policy and get the actions to take\n",
    "    policy_decision = inner_current_state.clone()\n",
    "    policy_decision[\"action\"] = torch.tensor(\n",
    "        [\n",
    "            [0 for _ in range(typed_cfg.dataset.env_kwargs[\"action-space\"])]\n",
    "            for _ in range(dataset.config.frames_per_update)\n",
    "        ]\n",
    "    )\n",
    "    policy_decision[\"action\"][:, a] = 1\n",
    "\n",
    "    # step the environmet\n",
    "    transitions, inner_current_state = envs.step_and_maybe_reset(policy_decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e18142",
   "metadata": {},
   "source": [
    "# Random actions from initialised policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2af7835",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_from_buffer = next(iter(dataset))\n",
    "\n",
    "# parallel: 67.6s, ~25 is spinning up; serial is 159.8s, for 128,000 frames on 32 envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc21e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8810b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_shape = (11, 13)\n",
    "idx = 24\n",
    "t = tensor_to_string(sampled_from_buffer[idx][\"inputs\"].reshape(dg_shape).T)\n",
    "print(\"Before:\\n\")\n",
    "print(\"\\n\".join(t))\n",
    "print(\"\\n\")\n",
    "print(\"Action:\", action_one_hot_to_string(sampled_from_buffer[idx][\"action\"]))\n",
    "print(\"\\n\")\n",
    "print(\"After:\\n\")\n",
    "t2 = tensor_to_string(sampled_from_buffer[idx][\"next\"][\"inputs\"].reshape(dg_shape).T)\n",
    "print(\"\\n\".join(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107aeb98",
   "metadata": {},
   "source": [
    "# South action\n",
    "\n",
    "test the internal code to check that resetting works predictably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93892d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.buffer.empty()\n",
    "buffer = TensorDictReplayBuffer(\n",
    "    batch_size=dataset.config.data_collection_batch_size,\n",
    "    storage=LazyTensorStorage(dataset.config.buffer_capacity),\n",
    "    prefetch=dataset.config.num_workers,\n",
    "    transform=lambda td: td.to(dataset.config.storing_device),\n",
    ")\n",
    "buffer.empty()\n",
    "envs = SerialEnv(dataset.config.frames_per_update, dataset.create_env)\n",
    "inner_current_state = envs.reset()\n",
    "\n",
    "for _ in tqdm(range(55)):\n",
    "    # pass the current state through to our policy and get the actions to take\n",
    "    policy_decision = inner_current_state.clone()\n",
    "    policy_decision[\"action\"] = torch.tensor(\n",
    "        [\n",
    "            [0 for _ in range(dataset.config.action_space_size)]\n",
    "            for _ in range(dataset.config.frames_per_update)\n",
    "        ]\n",
    "    )\n",
    "    policy_decision[\"action\"][:, input_action_map[\"s\"]] = 1\n",
    "\n",
    "    # step the environmet\n",
    "    transitions, inner_current_state = envs.step_and_maybe_reset(policy_decision)\n",
    "\n",
    "    # chug that in the buffer\n",
    "    buffer.extend(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc347fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = 10\n",
    "time_idx = 3\n",
    "buffer_idx = time_idx * dataset.config.frames_per_update + batch_idx\n",
    "\n",
    "dg_shape = (11, 13)\n",
    "t = tensor_to_string(buffer[buffer_idx][\"inputs\"].reshape(dg_shape).T)\n",
    "print(\"Before:\\n\")\n",
    "print(\"\\n\".join(t))\n",
    "print(\"\\n\")\n",
    "print(\"Reward:\", buffer[buffer_idx][\"next\"][\"reward\"].item())\n",
    "print(\"Action:\", action_one_hot_to_string(buffer[buffer_idx][\"action\"]))\n",
    "print(\"\\n\")\n",
    "print(\"After:\\n\")\n",
    "t2 = tensor_to_string(buffer[buffer_idx][\"next\"][\"inputs\"].reshape(dg_shape).T)\n",
    "print(\"\\n\".join(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea79b5",
   "metadata": {},
   "source": [
    "# Test validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2896db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_trajectories = dataset.validation_rollout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d768f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_trajectories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
